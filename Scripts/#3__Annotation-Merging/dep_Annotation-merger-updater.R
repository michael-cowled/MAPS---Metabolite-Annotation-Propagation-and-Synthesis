## Annotation Merger (Fix for dataset.ID warning) ##

library(dplyr)
library(readr)
library(tools)

data_folder <- "Y:/MA_BPA_Microbiome/Dataset-Annotations"
output_file_path <- "Y:/MA_BPA_Microbiome/Total-List-Of-Annotations.csv"

# Placeholder for your fix_compound_names function
# Make sure this function is defined and accessible in your environment

# Define standard columns and their types for consistent reading
# IMPORTANT: dataset.ID is included here because it's a final output column,
# but it's NOT expected to be in the input CSVs.
standard_col_types <- cols(
  feature.ID = col_character(),
  rt = col_double(),
  mz = col_double(),
  Best.Annotation = col_character(),
  Best.Annotation.Smiles = col_character(),
  Best.Annotation.Confidence.Score = col_double(),
  Best.Annotation.Confidence.Level = col_double(),
  Best.Annotation.Type = col_character(),
  dataset.ID = col_character() # Ensure dataset.ID is always character
)
# We'll use this list of column names for `select()` to ensure order and presence
standard_cols <- names(standard_col_types$cols)

# Define columns *expected to be in the input CSV files*
# This excludes dataset.ID as it's generated by the script
input_required_cols <- c(
  "feature.ID", "rt", "mz", "Best.Annotation", "Best.Annotation.Smiles",
  "Best.Annotation.Confidence.Score", "Best.Annotation.Confidence.Level",
  "Best.Annotation.Type"
)


# --- Prepare existing data if it exists ---
if (file.exists(output_file_path)) {
  message("Existing output file found. Loading for update...")
  previous_output <- tryCatch({
    readr::read_csv(output_file_path, show_col_types = FALSE, col_types = standard_col_types)
  }, error = function(e) {
    message(paste("Error reading existing output file:", output_file_path, "-", e$message))
    message("Proceeding without previous data. A new file will be created.")
    return(NULL)
  })
  
  # Ensure previous_output has all standard columns, filling with NA if missing
  if (!is.null(previous_output)) {
    missing_cols_prev <- setdiff(standard_cols, names(previous_output))
    for(col in missing_cols_prev) {
      if(col %in% c("feature.ID", "Best.Annotation", "Best.Annotation.Smiles", "Best.Annotation.Type", "dataset.ID")) {
        previous_output[[col]] <- NA_character_
      } else if (col %in% c("rt", "mz", "Best.Annotation.Confidence.Score", "Best.Annotation.Confidence.Level")) {
        previous_output[[col]] <- NA_real_
      } else {
        previous_output[[col]] <- NA # Generic NA
      }
    }
    previous_output <- previous_output %>% select(all_of(standard_cols))
  }
  
} else {
  previous_output <- NULL
  message("No existing output file found. A new file will be created.")
}


# --- Initialize storage for all new entries (from current run) ---
all_new_entries <- tibble(
  feature.ID = character(),
  rt = numeric(),
  mz = numeric(),
  Best.Annotation = character(),
  Best.Annotation.Smiles = character(),
  Best.Annotation.Confidence.Score = numeric(),
  Best.Annotation.Confidence.Level = numeric(),
  Best.Annotation.Type = character(),
  dataset.ID = character()
)


# --- Process new files ---
file_list <- list.files(data_folder, full.names = TRUE, pattern = "\\.csv$")

if (length(file_list) == 0) {
  message("No CSV files found in the data folder. Exiting.")
  if (!is.null(previous_output)) {
    message("No new data to process, but previous output exists. Writing previous output back to file.")
    write.csv(previous_output, output_file_path, row.names = FALSE)
  }
  stop("No new data to process.")
}


for (file_path in file_list) {
  file_name <- basename(file_path)
  dataset_id <- tools::file_path_sans_ext(file_name) # This is where dataset_id is derived
  
  message(paste("Processing file:", file_name))
  
  dataset <- tryCatch({
    # Read new files. We only specify types for the columns expected in the input.
    # col_types for reading input files should NOT expect 'dataset.ID'
    readr::read_csv(file_path, show_col_types = FALSE,
                    col_types = cols(
                      feature.ID = col_character(),
                      rt = col_double(),
                      mz = col_double(),
                      Best.Annotation = col_character(),
                      Best.Annotation.Smiles = col_character(),
                      Best.Annotation.Confidence.Score = col_double(),
                      Best.Annotation.Confidence.Level = col_double(),
                      Best.Annotation.Type = col_character(),
                      .default = col_guess() # Let R guess other types not explicitly listed
                    ))
  }, error = function(e) {
    message(paste("Error reading file:", file_path, "-", e$message))
    return(NULL)
  })
  
  if (is.null(dataset)) {
    next
  }
  
  # Now, check for required columns in the input file (excluding dataset.ID)
  missing_cols_curr <- setdiff(input_required_cols, names(dataset)) # <--- FIX HERE
  if (length(missing_cols_curr) > 0) {
    message(paste("Warning: Missing required columns in", basename(file_path), ":", paste(missing_cols_curr, collapse = ", "), ". Skipping file."))
    next
  }
  
  # Add the 'dataset.ID' column after the required column check
  dataset$dataset.ID <- dataset_id # <--- dataset.ID is added here
  
  
  # Filter, prioritize by confidence, and select relevant columns from the current file
  current_processed_entries <- dataset %>%
    filter(!is.na(Best.Annotation.Smiles) & Best.Annotation.Smiles != "N/A") %>%
    filter(Best.Annotation.Confidence.Level <= 2) %>%
    # Sort to prioritize by confidence BEFORE taking distinct
    arrange(Best.Annotation.Smiles, Best.Annotation.Confidence.Level, desc(Best.Annotation.Confidence.Score)) %>%
    # Now take distinct, keeping the first (best confidence) entry for each SMILES within THIS file
    distinct(Best.Annotation.Smiles, .keep_all = TRUE) %>%
    select(all_of(standard_cols)) # Ensure selecting all standard columns for output
  
  all_new_entries <- bind_rows(all_new_entries, current_processed_entries)
}

# --- Combine and update: Add new unique entries from all_new_entries to previous_output ---

if (is.null(previous_output)) {
  final_combined_data <- all_new_entries
} else {
  # Concatenate old and new data
  combined_raw_data <- bind_rows(previous_output, all_new_entries)
  
  # Now, we process the full combined data to get the final unique set.
  # We want one row per Best.Annotation.Smiles, prioritizing by confidence.
  final_combined_data <- combined_raw_data %>%
    # Sort by SMILES, then confidence to ensure 'distinct' picks the best one
    arrange(Best.Annotation.Smiles, Best.Annotation.Confidence.Level, desc(Best.Annotation.Confidence.Score)) %>%
    # Get unique entries based on Best.Annotation.Smiles, keeping the best confidence values
    distinct(Best.Annotation.Smiles, .keep_all = TRUE) %>%
    select(all_of(standard_cols)) # Final selection to ensure consistency
}


# --- Final Processing and Output ---

# Apply your fix_compound_names function
if (exists("fix_compound_names") && is.function(fix_compound_names)) {
  final_combined_data <- fix_compound_names(final_combined_data, "Best.Annotation")
} else {
  message("Warning: 'fix_compound_names' function not found or not a function. Skipping compound name fix.")
}

# The final distinct on Best.Annotation:
# This step will make sure that if, after `fix_compound_names`, multiple different
# SMILES map to the same Best.Annotation string, only one of them is kept.
final_combined_unique_entries_filtered <- final_combined_data %>%
  distinct(Best.Annotation, .keep_all = TRUE)  %>%
  distinct(Best.Annotation.Smiles, .keep_all = TRUE) %>%
  distinct(Best.Annotation.SMILES, .keep_all = TRUE)      ##Just keep one, once updated


write.csv(final_combined_unique_entries_filtered, output_file_path, row.names = FALSE)

message(paste("Processing complete. Updated annotations saved to:", output_file_path))

summary_table <- final_combined_unique_entries_filtered %>%
     group_by(Best.Annotation.Confidence.Level) %>%
     summarise(count = n())
print(summary_table)