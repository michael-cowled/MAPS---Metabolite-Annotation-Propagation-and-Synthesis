#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0146"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#-----------------------------------------------------------------------------------------------------------------------#
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write_csv(data, file = paste0("HGM/", sheet, ".csv"))
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (!is.na(dataset$column.type[1]) && dataset$column.type[1] == "HILIC") {
rt.tol <- 0.2
} else {
rt.tol <- 0.1
}
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write_csv(file_info, file = output_path) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
#-----------------------------------------------------------------------------------------------------------------------#
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
#-----------------------------------------------------------------------------------------------------------------------#
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
# Setting up database and cache connection
library(DBI)
cid_db_con <- dbConnect(RSQLite::SQLite(), "~/PubChem_Indexed.sqlite")
cid_cache_path <- "~/cid_cache.csv"
if (file.exists(cid_cache_path)) {
cid_cache_df <- read.csv(cid_cache_path, stringsAsFactors = FALSE)
# Ensure all necessary columns exist
required_cols <- c("LookupName", "ResolvedName", "SMILES", "CID")
missing_cols <- setdiff(required_cols, names(cid_cache_df))
if (length(missing_cols) > 0) {
for (col in missing_cols) cid_cache_df[[col]] <- NA
}
cid_cache_df <- cid_cache_df[, required_cols]
} else {
message("[CACHE INIT] No cache file found. Initializing empty cache.")
cid_cache_df <- data.frame(LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
stringsAsFactors = FALSE)
}
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
# Call function
result <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_cache_df = cid_cache_df,
cid_database_path = "~/PubChem_Indexed.sqlite"
)
remove.packages("MAPS.Package")
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0146"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#-----------------------------------------------------------------------------------------------------------------------#
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write_csv(data, file = paste0("HGM/", sheet, ".csv"))
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (!is.na(dataset$column.type[1]) && dataset$column.type[1] == "HILIC") {
rt.tol <- 0.2
} else {
rt.tol <- 0.1
}
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write_csv(file_info, file = output_path) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
#-----------------------------------------------------------------------------------------------------------------------#
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
#-----------------------------------------------------------------------------------------------------------------------#
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
# Setting up database and cache connection
library(DBI)
cid_db_con <- dbConnect(RSQLite::SQLite(), "~/PubChem_Indexed.sqlite")
cid_cache_path <- "~/cid_cache.csv"
if (file.exists(cid_cache_path)) {
cid_cache_df <- read.csv(cid_cache_path, stringsAsFactors = FALSE)
# Ensure all necessary columns exist
required_cols <- c("LookupName", "ResolvedName", "SMILES", "CID")
missing_cols <- setdiff(required_cols, names(cid_cache_df))
if (length(missing_cols) > 0) {
for (col in missing_cols) cid_cache_df[[col]] <- NA
}
cid_cache_df <- cid_cache_df[, required_cols]
} else {
message("[CACHE INIT] No cache file found. Initializing empty cache.")
cid_cache_df <- data.frame(LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
stringsAsFactors = FALSE)
}
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
# Call function
result <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_cache_df = cid_cache_df,
cid_database_path = "~/PubChem_Indexed.sqlite"
)
remove.packages("MAPS.Package")
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0146"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0146"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#-----------------------------------------------------------------------------------------------------------------------#
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write_csv(data, file = paste0("HGM/", sheet, ".csv"))
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (!is.na(dataset$column.type[1]) && dataset$column.type[1] == "HILIC") {
rt.tol <- 0.2
} else {
rt.tol <- 0.1
}
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write_csv(file_info, file = output_path) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
#-----------------------------------------------------------------------------------------------------------------------#
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
#-----------------------------------------------------------------------------------------------------------------------#
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
# Setting up database and cache connection
library(DBI)
cid_db_con <- dbConnect(RSQLite::SQLite(), "~/PubChem_Indexed.sqlite")
cid_cache_path <- "~/cid_cache.csv"
if (file.exists(cid_cache_path)) {
cid_cache_df <- read.csv(cid_cache_path, stringsAsFactors = FALSE)
# Ensure all necessary columns exist
required_cols <- c("LookupName", "ResolvedName", "SMILES", "CID")
missing_cols <- setdiff(required_cols, names(cid_cache_df))
if (length(missing_cols) > 0) {
for (col in missing_cols) cid_cache_df[[col]] <- NA
}
cid_cache_df <- cid_cache_df[, required_cols]
} else {
message("[CACHE INIT] No cache file found. Initializing empty cache.")
cid_cache_df <- data.frame(LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
stringsAsFactors = FALSE)
}
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
# Call function
result <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_cache_df = cid_cache_df,
cid_database_path = "~/PubChem_Indexed.sqlite"
)
View(lv2.annotations)
