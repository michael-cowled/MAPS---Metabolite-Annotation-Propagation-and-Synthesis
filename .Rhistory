mzmine.annotations$MonoisotopicMass[i] <- row_match$MonoisotopicMass[1]
}
}
mzmine.annotations.final <- mzmine.annotations %>%
group_by(id) %>%
# Add mzmine.id.prob = 1 / number of rows per id
mutate(mzmine.id.prob = 1 / n()) %>%
# Keep only the row(s) with the highest confidence.score per id
filter(score == max(score)) %>%
slice(1) %>%  # In case of ties, keep one arbitrarily
ungroup() %>%
# Keep only relevant columns:
select(id, compound_name, score, smiles, mzmine.id.prob, mol_formula, CID, MonoisotopicMass)
names(mzmine.annotations.final) <- c('feature.ID', "compound.name", "confidence.score",
"smiles", "id.prob", "molecular.formula", "CID", "MonoisotopicMass")
mzmine.annotations.final$feature.ID <- as.numeric(mzmine.annotations.final$feature.ID)
mzmine.annotations.final$confidence.level <- "1"
mzmine.annotations.final$annotation.type <- "authentic standard"
#Now appending to all mzmine features
mzmine.data <- read.csv(mzmine.data) # Derived from Export to CSV file (modular)
sample.data <- mzmine.data # A copy to be used for different processing later
if (!("spectral_db_matches.compound_name" %in% names(mzmine.data))) {
mzmine.data$spectral_db_matches.compound_name <- NA
}
mzmine.data <- select(mzmine.data, "id", "rt", "mz", "ion_identities.iin_id") %>%
mutate(across(everything(), as.character)) %>%
mutate(across(everything(), ~ na_if(., "")))
names(mzmine.data) <- c('feature.ID', "rt", "mz", "ion.identity.ID")
mzmine.data$feature.ID <- as.numeric(mzmine.data$feature.ID)
mzmine.data <- mzmine.data %>%
left_join(mzmine.annotations.final, by = "feature.ID")
## 6. Load, tidy and standardise GNPS2 data - works for v0.1.2, no metadata required
gnps.annotation.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/library/merged_results_with_gnps.tsv"))
gnps.annotation.data <- gnps.annotation.data[, c(2, 4, 5, 8, 9, 15, 27, 35, 43, 45, 46)]
names(gnps.annotation.data) <- c("feature.ID", "library.name", "confidence.score",
"mz.diff.ppm", "gnps.shared.peaks", "compound.name",
"smiles", "library.quality", "NPC.superclass", "NPC.pathway", "gnps.library.usi")
gnps.annotation.data$gnps.library.usi <- str_replace(
gnps.annotation.data$gnps.library.usi,
"mzspec:GNPS:GNPS-LIBRARY:(.*)",
"mzspec:GNPS:GNPS-LIBRARY:accession:\\1"
)
gnps.annotation.data$gnps.in.silico.bile.acid.info <- NA
gnps.annotation.data$gnps.in.silico.bile.acid.info[grepl("Candidate ", gnps.annotation.data$compound.name)] <-
gnps.annotation.data$compound.name[grepl("Candidate ", gnps.annotation.data$compound.name)]
gnps.annotation.data <- gnps.annotation.data[, c(1, 6, 7, 3:5, 2, 8, 10, 9, 11, 12)] #Reorders columns
gnps.annotation.data <- gnps.annotation.data %>%
filter(mz.diff.ppm <= 5) %>%
filter(confidence.score >= gnps.prob)
gnps.cluster.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/clustersummary_with_network.tsv"))
gnps.cluster.data <- select(gnps.cluster.data,  'cluster index', 'component')
names(gnps.cluster.data) <- c('feature.ID', "gnps.cluster.ID")
gnps.cluster.pairs <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/filtered_pairs.tsv"))
gnps.annotation.data$annotation.type <- "gnps"
#Create level 2, and tidy
gnps.data.lv2 <- filter(gnps.annotation.data, library.quality != "Insilico")
gnps.data.lv2$confidence.level <- "2"
gnps.data.lv2 <- fix_compound_names(gnps.data.lv2, "compound.name")
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#!# Run 1-Functions first! #!!#
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0070"     #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC)
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
# --- Load or Initialize Cache ---
cid_cache_file <- "cid_cache.csv"
if (file.exists(cid_cache_file)) {
cid_cache_df <- read.csv(cid_cache_file, stringsAsFactors = FALSE)
} else {
cid_cache_df <- data.frame(
LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
MolecularFormula = character(),
MonoisotopicMass = numeric(),
stringsAsFactors = FALSE
)
}
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write.csv(data, file = paste0("HGM/", sheet, ".csv"), row.names = FALSE)
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write.csv(file_info, file = output_path, row.names = FALSE) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id
filter(score > min(score) | n() == 1) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score)) %>%
slice(1) %>%  # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations <- standardise_annotation(mzmine.annotations, "compound_name", "smiles")
write.csv(cid_cache_df, cid_cache_file, row.names = FALSE)
# --- Populate from cache ---
for (i in seq_len(nrow(mzmine.annotations))) {
current_name <- mzmine.annotations$compound_name[i]
row_match <- cid_cache_df[cid_cache_df$LookupName == current_name, ]
if (nrow(row_match) > 0) {
mzmine.annotations$CID[i] <- row_match$CID[1]
mzmine.annotations$smiles[i] <- row_match$SMILES[1]
mzmine.annotations$mol_formula[i] <- row_match$MolecularFormula[1]
mzmine.annotations$MonoisotopicMass[i] <- row_match$MonoisotopicMass[1]
}
}
mzmine.annotations.final <- mzmine.annotations %>%
group_by(id) %>%
# Add mzmine.id.prob = 1 / number of rows per id
mutate(mzmine.id.prob = 1 / n()) %>%
# Keep only the row(s) with the highest confidence.score per id
filter(score == max(score)) %>%
slice(1) %>%  # In case of ties, keep one arbitrarily
ungroup() %>%
# Keep only relevant columns:
select(id, compound_name, score, smiles, mzmine.id.prob, mol_formula, CID, MonoisotopicMass)
names(mzmine.annotations.final) <- c('feature.ID', "compound.name", "confidence.score",
"smiles", "id.prob", "molecular.formula", "CID", "MonoisotopicMass")
mzmine.annotations.final$feature.ID <- as.numeric(mzmine.annotations.final$feature.ID)
mzmine.annotations.final$confidence.level <- "1"
mzmine.annotations.final$annotation.type <- "authentic standard"
#Now appending to all mzmine features
mzmine.data <- read.csv(mzmine.data) # Derived from Export to CSV file (modular)
sample.data <- mzmine.data # A copy to be used for different processing later
if (!("spectral_db_matches.compound_name" %in% names(mzmine.data))) {
mzmine.data$spectral_db_matches.compound_name <- NA
}
mzmine.data <- select(mzmine.data, "id", "rt", "mz", "ion_identities.iin_id") %>%
mutate(across(everything(), as.character)) %>%
mutate(across(everything(), ~ na_if(., "")))
names(mzmine.data) <- c('feature.ID', "rt", "mz", "ion.identity.ID")
mzmine.data$feature.ID <- as.numeric(mzmine.data$feature.ID)
mzmine.data <- mzmine.data %>%
left_join(mzmine.annotations.final, by = "feature.ID")
## 6. Load, tidy and standardise GNPS2 data - works for v0.1.2, no metadata required
gnps.annotation.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/library/merged_results_with_gnps.tsv"))
gnps.annotation.data <- gnps.annotation.data[, c(2, 4, 5, 8, 9, 15, 27, 35, 43, 45, 46)]
names(gnps.annotation.data) <- c("feature.ID", "library.name", "confidence.score",
"mz.diff.ppm", "gnps.shared.peaks", "compound.name",
"smiles", "library.quality", "NPC.superclass", "NPC.pathway", "gnps.library.usi")
gnps.annotation.data$gnps.library.usi <- str_replace(
gnps.annotation.data$gnps.library.usi,
"mzspec:GNPS:GNPS-LIBRARY:(.*)",
"mzspec:GNPS:GNPS-LIBRARY:accession:\\1"
)
gnps.annotation.data$gnps.in.silico.bile.acid.info <- NA
gnps.annotation.data$gnps.in.silico.bile.acid.info[grepl("Candidate ", gnps.annotation.data$compound.name)] <-
gnps.annotation.data$compound.name[grepl("Candidate ", gnps.annotation.data$compound.name)]
gnps.annotation.data <- gnps.annotation.data[, c(1, 6, 7, 3:5, 2, 8, 10, 9, 11, 12)] #Reorders columns
gnps.annotation.data <- gnps.annotation.data %>%
filter(mz.diff.ppm <= 5) %>%
filter(confidence.score >= gnps.prob)
gnps.cluster.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/clustersummary_with_network.tsv"))
gnps.cluster.data <- select(gnps.cluster.data,  'cluster index', 'component')
names(gnps.cluster.data) <- c('feature.ID', "gnps.cluster.ID")
gnps.cluster.pairs <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/filtered_pairs.tsv"))
gnps.annotation.data$annotation.type <- "gnps"
#Create level 2, and tidy
gnps.data.lv2 <- filter(gnps.annotation.data, library.quality != "Insilico")
gnps.data.lv2$confidence.level <- "2"
gnps.data.lv2 <- fix_compound_names(gnps.data.lv2, "compound.name")
print(MAPS.Package::fix_compound_names)
gnps.data.lv2 <- fix_compound_names(gnps.data.lv2, "compound.name")
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#!# Run 1-Functions first! #!!#
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0070"     #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC)
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
# --- Load or Initialize Cache ---
cid_cache_file <- "cid_cache.csv"
if (file.exists(cid_cache_file)) {
cid_cache_df <- read.csv(cid_cache_file, stringsAsFactors = FALSE)
} else {
cid_cache_df <- data.frame(
LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
MolecularFormula = character(),
MonoisotopicMass = numeric(),
stringsAsFactors = FALSE
)
}
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write.csv(data, file = paste0("HGM/", sheet, ".csv"), row.names = FALSE)
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write.csv(file_info, file = output_path, row.names = FALSE) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id
filter(score > min(score) | n() == 1) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score)) %>%
slice(1) %>%  # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations <- standardise_annotation(mzmine.annotations, "compound_name", "smiles")
write.csv(cid_cache_df, cid_cache_file, row.names = FALSE)
# --- Populate from cache ---
for (i in seq_len(nrow(mzmine.annotations))) {
current_name <- mzmine.annotations$compound_name[i]
row_match <- cid_cache_df[cid_cache_df$LookupName == current_name, ]
if (nrow(row_match) > 0) {
mzmine.annotations$CID[i] <- row_match$CID[1]
mzmine.annotations$smiles[i] <- row_match$SMILES[1]
mzmine.annotations$mol_formula[i] <- row_match$MolecularFormula[1]
mzmine.annotations$MonoisotopicMass[i] <- row_match$MonoisotopicMass[1]
}
}
mzmine.annotations.final <- mzmine.annotations %>%
group_by(id) %>%
# Add mzmine.id.prob = 1 / number of rows per id
mutate(mzmine.id.prob = 1 / n()) %>%
# Keep only the row(s) with the highest confidence.score per id
filter(score == max(score)) %>%
slice(1) %>%  # In case of ties, keep one arbitrarily
ungroup() %>%
# Keep only relevant columns:
select(id, compound_name, score, smiles, mzmine.id.prob, mol_formula, CID, MonoisotopicMass)
names(mzmine.annotations.final) <- c('feature.ID', "compound.name", "confidence.score",
"smiles", "id.prob", "molecular.formula", "CID", "MonoisotopicMass")
mzmine.annotations.final$feature.ID <- as.numeric(mzmine.annotations.final$feature.ID)
mzmine.annotations.final$confidence.level <- "1"
mzmine.annotations.final$annotation.type <- "authentic standard"
#Now appending to all mzmine features
mzmine.data <- read.csv(mzmine.data) # Derived from Export to CSV file (modular)
sample.data <- mzmine.data # A copy to be used for different processing later
if (!("spectral_db_matches.compound_name" %in% names(mzmine.data))) {
mzmine.data$spectral_db_matches.compound_name <- NA
}
mzmine.data <- select(mzmine.data, "id", "rt", "mz", "ion_identities.iin_id") %>%
mutate(across(everything(), as.character)) %>%
mutate(across(everything(), ~ na_if(., "")))
names(mzmine.data) <- c('feature.ID', "rt", "mz", "ion.identity.ID")
mzmine.data$feature.ID <- as.numeric(mzmine.data$feature.ID)
mzmine.data <- mzmine.data %>%
left_join(mzmine.annotations.final, by = "feature.ID")
## 6. Load, tidy and standardise GNPS2 data - works for v0.1.2, no metadata required
gnps.annotation.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/library/merged_results_with_gnps.tsv"))
gnps.annotation.data <- gnps.annotation.data[, c(2, 4, 5, 8, 9, 15, 27, 35, 43, 45, 46)]
names(gnps.annotation.data) <- c("feature.ID", "library.name", "confidence.score",
"mz.diff.ppm", "gnps.shared.peaks", "compound.name",
"smiles", "library.quality", "NPC.superclass", "NPC.pathway", "gnps.library.usi")
gnps.annotation.data$gnps.library.usi <- str_replace(
gnps.annotation.data$gnps.library.usi,
"mzspec:GNPS:GNPS-LIBRARY:(.*)",
"mzspec:GNPS:GNPS-LIBRARY:accession:\\1"
)
gnps.annotation.data$gnps.in.silico.bile.acid.info <- NA
gnps.annotation.data$gnps.in.silico.bile.acid.info[grepl("Candidate ", gnps.annotation.data$compound.name)] <-
gnps.annotation.data$compound.name[grepl("Candidate ", gnps.annotation.data$compound.name)]
gnps.annotation.data <- gnps.annotation.data[, c(1, 6, 7, 3:5, 2, 8, 10, 9, 11, 12)] #Reorders columns
gnps.annotation.data <- gnps.annotation.data %>%
filter(mz.diff.ppm <= 5) %>%
filter(confidence.score >= gnps.prob)
gnps.cluster.data <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/clustersummary_with_network.tsv"))
gnps.cluster.data <- select(gnps.cluster.data,  'cluster index', 'component')
names(gnps.cluster.data) <- c('feature.ID', "gnps.cluster.ID")
gnps.cluster.pairs <- read_tsv(paste0("https://gnps2.org/resultfile?task=", gnps.task.id, "&file=nf_output/networking/filtered_pairs.tsv"))
gnps.annotation.data$annotation.type <- "gnps"
#Create level 2, and tidy
gnps.data.lv2 <- filter(gnps.annotation.data, library.quality != "Insilico")
gnps.data.lv2$confidence.level <- "2"
gnps.data.lv2 <- fix_compound_names(gnps.data.lv2, "compound.name")
df[grepl(patterns[i], get(col), ignore.case = TRUE),
(col) := gsub(patterns[i], replacements[i], get(col), ignore.case = TRUE)]
library(MAPS.Package)
gnps.data.lv2 <- fix_compound_names(gnps.data.lv2, "compound.name")
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#!# Run 1-Functions first! #!!#
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0070"     #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC)
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
# --- Load or Initialize Cache ---
cid_cache_file <- "cid_cache.csv"
if (file.exists(cid_cache_file)) {
cid_cache_df <- read.csv(cid_cache_file, stringsAsFactors = FALSE)
} else {
cid_cache_df <- data.frame(
LookupName = character(),
ResolvedName = character(),
SMILES = character(),
CID = numeric(),
MolecularFormula = character(),
MonoisotopicMass = numeric(),
stringsAsFactors = FALSE
)
}
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write.csv(data, file = paste0("HGM/", sheet, ".csv"), row.names = FALSE)
}
