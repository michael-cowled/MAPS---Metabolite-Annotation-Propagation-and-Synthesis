#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write.csv(file_info, file = output_path, row.names = FALSE) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
mzmine.annotations <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_database_path = "Y:/MA_BPA_Microbiome/MS Databases/PubChem/cid_lookup.sqlite",
cid_cache_path = "~cid_cache.csv"
)
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0108"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#-----------------------------------------------------------------------------------------------------------------------#
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write.csv(data, file = paste0("HGM/", sheet, ".csv"), row.names = FALSE)
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (!is.na(dataset$column.type[1]) && dataset$column.type[1] == "HILIC") {
rt.tol <- 0.2
} else {
rt.tol <- 0.1
}
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write.csv(file_info, file = output_path, row.names = FALSE) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
mzmine.annotations <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_database_path = "Y:/MA_BPA_Microbiome/MS Databases/PubChem/cid_lookup.sqlite",
cid_cache_path = "~cid_cache.csv"
)
# get_pubchem.R
#' Query PubChem for CIDs, Titles, or Properties
#'
#' Queries the PubChem REST API using a compound name, SMILES, or CID
#' to retrieve CIDs, compound titles, or molecular properties.
#'
#' @param query The input string to search for (compound name, SMILES, or CID).
#' @param type A character string: one of \code{"name"}, \code{"smiles"}, \code{"synonym"}, or \code{"cid"}.
#' @param property A character string specifying what to retrieve. Options:
#'   \itemize{
#'     \item \code{"cids"}: to retrieve the compound ID(s)
#'     \item \code{"title"}: to get the PubChem title from a CID
#'     \item \code{"properties"}: to fetch SMILES
#'   }
#' @param db_con An active DBI connection object to your SQLite database, or NULL if not available.
#'
#' @return Depending on the request type:
#'   \itemize{
#'     \item A numeric CID
#'     \item A character string (compound title)
#'     \item A list with \code{SMILES}
#'     \item Or \code{NA} on failure or no result
#'   }
#' @export
#'
#' @examples
#' \dontrun{
#' # Assuming a db_con is established for properties lookup
#' # get_pubchem("glucose", type = "name", property = "cids")
#' # get_pubchem("50-99-7", type = "name", property = "cids")
#' # get_pubchem("CCO", type = "smiles", property = "cids")
#' # get_pubchem("5793", type = "cid", property = "title")
#' # get_pubchem("5793", type = "cid", property = "properties", db_con = my_db_connection)
#' }
get_pubchem <- function(query, type, property = NULL, db_con = NULL) {
base_url <- "https://pubchem.ncbi.nlm.nih.gov/rest/pug"
# Helper function to safely extract text, returning NA if node not found or empty
safe_xml_text <- function(xml_obj, xpath) {
# Check if xml_obj is valid before trying to find nodes
if (is.null(xml_obj) || !inherits(xml_obj, "xml_document")) {
return(NA_character_)
}
node <- xml2::xml_find_first(xml_obj, xpath)
# Check if node exists (length > 0)
if (length(node) == 0) {
return(NA_character_)
}
text <- xml2::xml_text(node)
# Check for empty string result, which can happen if node exists but has no text
if (length(text) == 0 || nchar(text) == 0) {
return(NA_character_)
}
return(text)
}
if (type == "name" && property == "cids") {
url <- paste0(base_url, "/compound/name/", URLencode(query), "/cids/JSON")
response <- tryCatch(jsonlite::fromJSON(url), error = function(e) {
message(paste("  [get_pubchem ERROR] Name CID lookup failed for '", query, "':", e$message))
return(NULL)
})
if (!is.null(response$IdentifierList$CID)) {
Sys.sleep(0.2)
return(response$IdentifierList$CID[1])
} else return(NA_real_) # Return numeric NA for CID
} else if (type == "smiles" && property == "cids") {
url <- paste0(base_url, "/compound/smiles/", URLencode(query), "/cids/JSON")
response <- tryCatch(jsonlite::fromJSON(url), error = function(e) {
message(paste("  [get_pubchem ERROR] SMILES CID lookup failed for '", query, "':", e$message))
return(NULL)
})
if (!is.null(response$IdentifierList$CID)) {
Sys.sleep(0.2)
return(response$IdentifierList$CID[1])
} else return(NA_real_) # Return numeric NA for CID
} else if (type == "synonym" && property == "cids") {
url <- paste0(base_url, "/compound/name/", URLencode(query), "/synonyms/JSON")
response <- tryCatch(jsonlite::fromJSON(url), error = function(e) {
message(paste("  [get_pubchem ERROR] Synonym CID lookup failed for '", query, "':", e$message))
return(NULL)
})
if (!is.null(response$InformationList$Information[[1]]$CID)) {
Sys.sleep(0.2)
return(response$InformationList$Information[[1]]$CID)
} else {
message("    Synonym search returned no CID.")
return(NA_real_) # Return numeric NA for CID
}
} else if (type == "cid" && property == "title") {
page_url <- paste0("https://pubchem.ncbi.nlm.nih.gov/compound/", query)
page <- tryCatch(xml2::read_html(page_url), error = function(e) {
message(paste("  [get_pubchem ERROR] Failed to retrieve title page for CID", query, ":", e$message))
return(NULL)
})
if (!is.null(page)) {
title_node <- rvest::html_node(page, "title")
if (length(title_node) > 0) {
title <- rvest::html_text(title_node)
return(gsub(" - PubChem", "", title, fixed = TRUE))
} else {
return(NA_character_) # Return character NA if title node not found
}
} else return(NA_character_) # Return character NA on page fetch failure
} else if (type == "cid" && property == "properties") {
message(paste("  [get_pubchem DEBUG] Inside 'cid' && 'properties' block for CID:", query))
message(paste("  [get_pubchem DEBUG] Is db_con NULL?", is.null(db_con)))
if (!is.null(db_con)) {
message(paste("  [get_pubchem DEBUG] Is db_con valid?", DBI::dbIsValid(db_con)))
} else {
message("  [get_pubchem DEBUG] db_con is NULL, so not checking dbIsValid.")
}
# Use the local database to retrieve SMILES
if (!is.null(db_con) && DBI::dbIsValid(db_con)) {
tryCatch({
query_db <- sprintf("SELECT SMILES FROM cid_data WHERE CID = %s", sQuote(query))
message(paste("  [get_pubchem DEBUG] Executing DB query:", query_db))
db_result <- DBI::dbGetQuery(db_con, query_db)
if (nrow(db_result) > 0 && !is.na(db_result$SMILES[1])) {
message(paste("  [get_pubchem INFO] Retrieved SMILES from local DB for CID", query))
return(list(SMILES = db_result$SMILES[1]))
} else {
message(paste("  [get_pubchem INFO] SMILES not found in local DB for CID", query, ". Falling back to PubChem API."))
# Fallback to PubChem API if not found in local DB
url <- paste0(base_url, "/compound/cid/", query, "/property/IsomericSMILES/XML")
xml_response <- tryCatch({
xml2::read_xml(url)
}, error = function(e) {
message(paste("  [get_pubchem ERROR] Failed to retrieve properties from PubChem API for CID", query, ":", e$message))
return(NULL)
})
if (!is.null(xml_response) && !inherits(xml_response, "try-error")) {
smiles_from_pubchem <- safe_xml_text(xml_response, ".//IsomericSMILES")
Sys.sleep(0.2)
return(list(SMILES = smiles_from_pubchem))
} else {
return(NULL) # Indicate failure if both local DB and PubChem API fail
}
}
}, error = function(e) {
message(paste("  [get_pubchem ERROR] Database lookup failed for CID", query, ":", e$message))
# Fallback to PubChem API on database error
url <- paste0(base_url, "/compound/cid/", query, "/property/IsomericSMILES/XML")
xml_response <- tryCatch({
xml2::read_xml(url)
}, error = function(e) {
message(paste("  [get_pubchem ERROR] Failed to retrieve properties from PubChem API (after DB error) for CID", query, ":", e$message))
return(NULL)
})
if (!is.null(xml_response) && !inherits(xml_response, "try-error")) {
smiles_from_pubchem <- safe_xml_text(xml_response, ".//IsomericSMILES")
Sys.sleep(0.2)
return(list(SMILES = smiles_from_pubchem))
} else {
return(NULL)
}
})
} else {
# This is the path taken if (!is.null(db_con) && DBI::dbIsValid(db_con)) is FALSE
message(paste("  [get_pubchem INFO] No valid database connection for properties lookup. Querying PubChem API for CID", query))
# Original PubChem API call if no database connection is provided or valid
url <- paste0(base_url, "/compound/cid/", query, "/property/IsomericSMILES/XML")
response <- tryCatch({
xml <- xml2::read_xml(url)
if (is.null(xml) || inherits(xml, "try-error")) {
stop("XML object is NULL or an error after read_xml, forcing NULL return.")
}
result <- list(
SMILES = safe_xml_text(xml, ".//IsomericSMILES")
)
Sys.sleep(0.2)
return(result)
}, error = function(e) {
message(paste("  [get_pubchem ERROR] Failed to retrieve properties for CID", query, ":", e$message))
return(NULL)
})
return(response)
}
} else {
message("Invalid query type or property.")
return(NA)
}
}
#### ANNOTATION TABLE SCRIPT FOR AUSTRALIAN HUMAN GUT METABOLOME DATABASE ####
#------------------------------------------------------------------------------#
###---START OF USER-FED INFORMATION---###
# Dataset ID: From Data Management Plan:
dataset.id <- "HGMD_0108"      #####Change to dataset of interest#####
# Specify the path to the Data Management Plan (unless moved to Mediaflux then should be consistent)
excel_file <- "C:/Users/mcowled/The University of Melbourne/MA Human Gut Metabolome - Documents/Data Management and Analysis/HGM - Data Management System.xlsx"
# Annotation Acceptance Probabilities (Defaults for Exploratory Analyses)
# Increase stringency if the application demands it.
gnps.prob <- 0.7              # Default is 0.7 (and should be changed to those set in run)
canopus.prob <- 0.7           # Default is 0.7
csi.prob <- 0.64            # Default is 0.64 ##10% FDR
ms2query.prob <- 0.63         # Default is 0.63 ##Recommended by paper
# Specify rt tolerance of standards (min)
# rt.tol <- 0.1               # Default is 0.1 min for C18 (use 0.2 min for HILIC) ##More important for public release
###---END OF USER-FED INFORMATION---###
#------------------------------------------------------------------------------#
## 1. check_and_install
setwd(file.path("~/"))
# Function to check, install, and load required packages
check_and_install <- function(packages, github_packages = list()) {
# Install 'remotes' if needed for GitHub installs
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
for (pkg in packages) {
if (!requireNamespace(pkg, quietly = TRUE)) {
if (pkg %in% names(github_packages)) {
message(paste("Installing", pkg, "from GitHub:", github_packages[[pkg]]))
remotes::install_github(github_packages[[pkg]])
} else {
message(paste("Installing", pkg, "from CRAN"))
install.packages(pkg, dependencies = TRUE)
}
}
suppressPackageStartupMessages(library(pkg, character.only = TRUE))
}
}
required_packages <- c("MAPS.Package", "dplyr", "tidyr", "stringr", "readr",
"reshape2", "ggplot2", "svglite", "readxl", "data.table",
"openxlsx", "tidyverse", "rvest", "jsonlite", "xml2", "progress",
"DBI", "RSQLite")
github_packages <- list("MAPS.Package" = "michael-cowled/MAPS-Package-Public")
check_and_install(required_packages, github_packages)
#-----------------------------------------------------------------------------------------------------------------------#
## 2. Updating Metadata and extract gnps task ID                                ----> Remove from published version
sheet_names <- excel_sheets(excel_file) # Get the sheet names
for (sheet in sheet_names) {
data <- read_excel(excel_file, sheet = sheet)
write.csv(data, file = paste0("HGM/", sheet, ".csv"), row.names = FALSE)
}
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
gnps.task.id <- dataset$gnps.task.ID[1] #                                       ----> Put gnps.task.ID as a required user-fed info in published
if (!is.na(dataset$column.type[1]) && dataset$column.type[1] == "HILIC") {
rt.tol <- 0.2
} else {
rt.tol <- 0.1
}
if (is.na(gnps.task.id)) {
stop("gnps.task.ID is missing. Add to 'dataset' csv before re-running.")  # Stop execution
}
#-----------------------------------------------------------------------------------------------------------------------#
## 3. File List Extractor                                                       ----> Remove from published version
dataset <- read.csv("HGM/D - Dataset.csv") %>%
filter(HGMD.ID == dataset.id)
output_directory <- dataset$Processed.Data.Folder[1] # Take the first value if there are multiple
folder <- paste0(dataset$Processed.Data.Folder, "\\mzml")
print(folder)
if (dir.exists(folder)) {
files <- list.files(folder)
full_paths <- file.path(folder, files)
full_paths_windows <- gsub("/", "\\\\", full_paths)
file_info <- data.frame(
folder = folder,
filename = files,
full_path = full_paths_windows
)
print(file_info)
output_filename <- paste0(dataset.id, "_file_list.csv")
output_path <- file.path(output_directory, output_filename) # changed here
write.csv(file_info, file = output_path, row.names = FALSE) # and here
print(paste("File list saved to:", output_path)) # and here
} else {
print("Folder does not exist. Check the path.")
}
## 4. Processed Data Check
#Check if all data is present in folder with correct naming conventions         ###CHECK SPELLING!!!!###
#Folder with mzmine, ms2query, sirius and gnps results
folder <- dataset$Processed.Data.Folder[1]
# Run validation and collect paths
paths <- validate_and_get_paths(folder)
# Access each file path as needed
mzmine.data <- paths$mzmine_data
mzmine.annotations <- paths$mzmine_annotations
canopus.data <- paths$canopus_data
csi.data <- paths$csi_data
zodiac.data <- paths$zodiac_data
ms2query.data <- paths$ms2query_data
cytoscape <- paths$cytoscape
## 5. Load in MZMINE data
mzmine.annotations <- read.csv(mzmine.annotations) %>%
# First, ensure distinct compound_name per id
distinct(id, compound_name, .keep_all = TRUE) %>%
group_by(id) %>%
# Remove all rows with the lowest confidence.score per id,
# or keep the first if all scores within the group are identical.
filter(
n() == 1 |                                   # Keep if there's only one row in the group
score > min(score, na.rm = TRUE) |           # Keep if score is strictly greater than the min score in the group
(all(score == min(score, na.rm = TRUE)) &    # If all scores are identical to the min score, AND
row_number() == 1)                          # Keep only the first row
) %>%
ungroup() %>%
# Now, reduce to only one row per compound_name (highest confidence.score overall)
group_by(compound_name) %>%
filter(score == max(score, na.rm = TRUE)) %>%
slice(1) %>%                                   # If there's a tie on confidence.score, pick the first row arbitrarily
ungroup()
#Calculating ID probability of level 1 annotations
mzmine.annotations$mzmine.id.prob <- NA
#Standardisation of compound names (retrieving from a locally stored cache/pubchem)
# --- Main Processing Loop ---
mzmine.annotations$smiles <- trimws(mzmine.annotations$smiles)
mzmine.annotations <- standardise_annotation(
mzmine.annotations,
name_col = "compound_name",
smiles_col = "smiles",
cid_database_path = "Y:/MA_BPA_Microbiome/MS Databases/PubChem/cid_lookup.sqlite",
cid_cache_path = "~cid_cache.csv"
)
